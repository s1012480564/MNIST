{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "python中的路径用 / \\ 均可，但是 \\ 是转义\n",
    "下面几种写法等价\n",
    "'kaggle_data/train.csv'\n",
    "'kaggle_data\\\\\\\\train.csv'\n",
    "r'kaggle_data\\train.csv'\n",
    "r 就是 raw string 的意思，即不转义"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0          1       0       0       0       0       0       0       0       0   \n1          0       0       0       0       0       0       0       0       0   \n2          1       0       0       0       0       0       0       0       0   \n3          4       0       0       0       0       0       0       0       0   \n4          0       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n41995      0       0       0       0       0       0       0       0       0   \n41996      1       0       0       0       0       0       0       0       0   \n41997      7       0       0       0       0       0       0       0       0   \n41998      6       0       0       0       0       0       0       0       0   \n41999      9       0       0       0       0       0       0       0       0   \n\n       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0           0  ...         0         0         0         0         0   \n1           0  ...         0         0         0         0         0   \n2           0  ...         0         0         0         0         0   \n3           0  ...         0         0         0         0         0   \n4           0  ...         0         0         0         0         0   \n...       ...  ...       ...       ...       ...       ...       ...   \n41995       0  ...         0         0         0         0         0   \n41996       0  ...         0         0         0         0         0   \n41997       0  ...         0         0         0         0         0   \n41998       0  ...         0         0         0         0         0   \n41999       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n0             0         0         0         0         0  \n1             0         0         0         0         0  \n2             0         0         0         0         0  \n3             0         0         0         0         0  \n4             0         0         0         0         0  \n...         ...       ...       ...       ...       ...  \n41995         0         0         0         0         0  \n41996         0         0         0         0         0  \n41997         0         0         0         0         0  \n41998         0         0         0         0         0  \n41999         0         0         0         0         0  \n\n[42000 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41996</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41997</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41998</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41999</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>42000 rows × 785 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df=pd.read_csv('kaggle_data/train.csv')\n",
    "test_df=pd.read_csv('kaggle_data/test.csv')\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "((42000, 785), (28000, 784))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape,test_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_df=train_df.astype(np.float32)\n",
    "test_df=test_df.astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "((42000, 784), (42000,))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_y=train_df.iloc[:,0]\n",
    "train_df_X=train_df.iloc[:,1:]\n",
    "train_df_X.shape,train_df_y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "((42000, 1, 28, 28), (28000, 1, 28, 28))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np_X=train_df_X.values\n",
    "test_np_X=test_df.values\n",
    "train_np_X=train_np_X.reshape([42000,1,28,28])\n",
    "test_np_X=test_np_X.reshape([28000,1,28,28])\n",
    "train_np_X.shape,test_np_X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n          0.47450984,  1.        , -0.26274508, -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        ,  0.49803925,\n          0.9607843 ,  0.9843137 , -0.27058822, -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -0.03529412,  0.94509804,\n          0.9843137 ,  0.30980396, -0.92156863, -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -0.372549  ,  0.9372549 ,  0.9843137 ,\n          0.6313726 , -0.8980392 , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -0.77254903,  0.62352943,  0.9843137 ,  0.84313726,\n         -0.3960784 , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -0.5764706 ,\n          0.6392157 ,  0.9843137 ,  0.9843137 , -0.3098039 ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -0.27058822,  0.99215686,\n          0.9843137 ,  0.8666667 ,  0.33333337, -0.8666667 ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -0.81960785,  0.64705884,  0.99215686,\n          0.9843137 ,  0.24705887, -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -0.8745098 ,  0.6392157 ,  0.9843137 ,  0.99215686,\n          0.88235295, -0.36470586, -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -0.7882353 ,  0.9843137 ,  0.9843137 ,  0.99215686,\n         -0.8980392 , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -0.84313726,\n          0.6156863 ,  0.99215686,  0.99215686,  0.5529412 ,\n         -0.94509804, -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        ,  0.3176471 ,\n          0.9843137 ,  0.9843137 ,  0.5372549 , -0.94509804,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -0.84313726,  0.5921569 ,\n          0.9843137 ,  0.94509804, -0.40392154, -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -0.827451  ,  0.47450984,  0.9843137 ,\n          0.92156863, -0.27058822, -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -0.19215685,  0.9843137 ,  0.9843137 ,\n          0.49803925, -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -0.30196077,  0.88235295,  0.9843137 ,  0.5294118 ,\n         -0.8039216 , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -0.88235295,\n          0.7254902 ,  0.9843137 ,  0.9843137 , -0.372549  ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -0.26274508,\n          0.9843137 ,  0.9843137 ,  0.9843137 , -0.26274508,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -0.30196077,\n          0.96862745,  0.9843137 ,  0.9607843 ,  0.02745104,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n          0.6784314 ,  0.70980394, -0.25490195, -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ],\n        [-1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ,\n         -1.        , -1.        , -1.        , -1.        ]]],\n      dtype=float32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np_X=(train_np_X/255-0.5)/0.5\n",
    "test_np_X=(test_np_X/255-0.5)/0.5\n",
    "train_np_X[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "       0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0\n0      0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n1      1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n2      0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n3      0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n4      1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n41995  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n41996  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n41997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n41998  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n41999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n\n[42000 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0.0</th>\n      <th>1.0</th>\n      <th>2.0</th>\n      <th>3.0</th>\n      <th>4.0</th>\n      <th>5.0</th>\n      <th>6.0</th>\n      <th>7.0</th>\n      <th>8.0</th>\n      <th>9.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41995</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41996</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>42000 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_y=pd.get_dummies(train_df_y)\n",
    "train_df_y=train_df_y.astype(np.float32)\n",
    "train_df_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(42000, 10)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np_y=train_df_y.values\n",
    "train_np_y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch\n",
    "train_X=torch.from_numpy(train_np_X)\n",
    "test_X=torch.from_numpy(test_np_X)\n",
    "train_y=torch.from_numpy(train_np_y)\n",
    "from torch.utils.data import TensorDataset\n",
    "train_set=TensorDataset(train_X,train_y)\n",
    "test_set=TensorDataset(test_X) # 可以打印一下，这个构造成的是第二个元素为空的 tuple"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader=DataLoader(train_set,batch_size=64,shuffle=True)\n",
    "test_loader=DataLoader(test_set,batch_size=64,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意 torch 的 dim 认为 dim=0 是矩阵最后一维，倒着顺序来的\n",
    "于是 dim=-1 就是第一维\n",
    "softmax(ts,dim=-1) 就是表示按第一维不同的进行归一化概率计算\n",
    "第一维是样本数嘛，所以就是写 dim=-1 就对了"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (out): Linear(in_features=84, out_features=10, bias=True)\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5,padding=2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.fc1=nn.Linear(400,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.out=nn.Linear(84,10)\n",
    "    def forward(self,x):\n",
    "        x=self.pool(torch.tanh(self.conv1(x)))\n",
    "        x=self.pool(torch.tanh(self.conv2(x)))\n",
    "        x=x.reshape(-1,self.num_flat_features(x))\n",
    "        x=torch.tanh(self.fc1(x))\n",
    "        x=torch.tanh(self.fc2(x))\n",
    "        x=torch.softmax(self.out(x),dim=-1)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size=x.size()[1:]\n",
    "        num_features=1\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "cnn=CNN()\n",
    "cnn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')\n",
    "cnn=cnn.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(cnn.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] cost: 1045.762\n",
      "[2] cost: 981.092\n",
      "[3] cost: 975.881\n",
      "[4] cost: 972.568\n",
      "[5] cost: 970.414\n",
      "[6] cost: 969.387\n",
      "[7] cost: 968.708\n",
      "[8] cost: 967.167\n",
      "[9] cost: 966.498\n",
      "[10] cost: 965.890\n",
      "[11] cost: 965.575\n",
      "[12] cost: 964.629\n",
      "[13] cost: 964.920\n",
      "[14] cost: 964.499\n",
      "[15] cost: 964.121\n",
      "[16] cost: 963.984\n",
      "[17] cost: 963.399\n",
      "[18] cost: 963.554\n",
      "[19] cost: 963.339\n",
      "[20] cost: 963.554\n",
      "[21] cost: 962.917\n",
      "[22] cost: 963.016\n",
      "[23] cost: 963.068\n",
      "[24] cost: 963.283\n",
      "[25] cost: 963.028\n",
      "[26] cost: 962.766\n",
      "[27] cost: 962.343\n",
      "[28] cost: 962.632\n",
      "[29] cost: 962.142\n",
      "[30] cost: 962.449\n",
      "[31] cost: 962.634\n",
      "[32] cost: 962.150\n",
      "[33] cost: 961.889\n",
      "[34] cost: 962.645\n",
      "[35] cost: 962.625\n",
      "[36] cost: 962.297\n",
      "[37] cost: 962.407\n",
      "[38] cost: 962.591\n",
      "[39] cost: 962.065\n",
      "[40] cost: 962.123\n",
      "[41] cost: 962.594\n",
      "[42] cost: 962.333\n",
      "[43] cost: 962.083\n",
      "[44] cost: 961.456\n",
      "[45] cost: 961.907\n",
      "[46] cost: 962.465\n",
      "[47] cost: 962.113\n",
      "[48] cost: 961.929\n",
      "[49] cost: 961.724\n",
      "[50] cost: 961.362\n",
      "[51] cost: 962.320\n",
      "[52] cost: 962.000\n",
      "[53] cost: 961.858\n",
      "[54] cost: 961.402\n",
      "[55] cost: 962.320\n",
      "[56] cost: 961.724\n",
      "[57] cost: 961.192\n",
      "[58] cost: 961.475\n",
      "[59] cost: 961.955\n",
      "[60] cost: 961.936\n",
      "[61] cost: 961.519\n",
      "[62] cost: 961.370\n",
      "[63] cost: 961.895\n",
      "[64] cost: 961.661\n",
      "[65] cost: 961.739\n",
      "[66] cost: 961.206\n",
      "[67] cost: 961.590\n",
      "[68] cost: 961.338\n",
      "[69] cost: 961.387\n",
      "[70] cost: 961.727\n",
      "[71] cost: 961.625\n",
      "[72] cost: 961.705\n",
      "[73] cost: 961.682\n",
      "[74] cost: 961.381\n",
      "[75] cost: 961.086\n",
      "[76] cost: 961.763\n",
      "[77] cost: 961.309\n",
      "[78] cost: 961.225\n",
      "[79] cost: 961.138\n",
      "[80] cost: 961.444\n",
      "[81] cost: 961.898\n",
      "[82] cost: 962.068\n",
      "[83] cost: 961.504\n",
      "[84] cost: 961.590\n",
      "[85] cost: 961.133\n",
      "[86] cost: 961.359\n",
      "[87] cost: 961.309\n",
      "[88] cost: 961.220\n",
      "[89] cost: 961.076\n",
      "[90] cost: 960.908\n",
      "[91] cost: 961.291\n",
      "[92] cost: 961.905\n",
      "[93] cost: 961.441\n",
      "[94] cost: 961.325\n",
      "[95] cost: 961.164\n",
      "[96] cost: 961.540\n",
      "[97] cost: 961.298\n",
      "[98] cost: 961.156\n",
      "[99] cost: 961.449\n",
      "[100] cost: 961.043\n",
      "[101] cost: 961.552\n",
      "[102] cost: 961.414\n",
      "[103] cost: 961.067\n",
      "[104] cost: 961.146\n",
      "[105] cost: 961.305\n",
      "[106] cost: 961.251\n",
      "[107] cost: 961.423\n",
      "[108] cost: 961.448\n",
      "[109] cost: 961.279\n",
      "[110] cost: 961.105\n",
      "[111] cost: 961.459\n",
      "[112] cost: 961.275\n",
      "[113] cost: 961.232\n",
      "[114] cost: 961.435\n",
      "[115] cost: 961.419\n",
      "[116] cost: 961.086\n",
      "[117] cost: 961.212\n",
      "[118] cost: 961.026\n",
      "[119] cost: 961.011\n",
      "[120] cost: 961.191\n",
      "[121] cost: 961.270\n",
      "[122] cost: 961.608\n",
      "[123] cost: 961.417\n",
      "[124] cost: 961.263\n",
      "[125] cost: 961.140\n",
      "[126] cost: 961.124\n",
      "[127] cost: 961.248\n",
      "[128] cost: 961.288\n",
      "[129] cost: 961.140\n",
      "[130] cost: 961.274\n",
      "[131] cost: 961.228\n",
      "[132] cost: 961.474\n",
      "[133] cost: 961.022\n",
      "[134] cost: 961.239\n",
      "[135] cost: 961.318\n",
      "[136] cost: 961.078\n",
      "[137] cost: 960.960\n",
      "[138] cost: 961.080\n",
      "[139] cost: 961.225\n",
      "[140] cost: 961.100\n",
      "[141] cost: 961.033\n",
      "[142] cost: 961.312\n",
      "[143] cost: 961.225\n",
      "[144] cost: 961.524\n",
      "[145] cost: 960.928\n",
      "[146] cost: 961.297\n",
      "[147] cost: 961.325\n",
      "[148] cost: 961.311\n",
      "[149] cost: 961.211\n",
      "[150] cost: 961.416\n",
      "[151] cost: 960.972\n",
      "[152] cost: 961.394\n",
      "[153] cost: 961.471\n",
      "[154] cost: 961.140\n",
      "[155] cost: 961.312\n",
      "[156] cost: 961.189\n",
      "[157] cost: 961.311\n",
      "[158] cost: 961.012\n",
      "[159] cost: 961.139\n",
      "[160] cost: 961.275\n",
      "[161] cost: 961.109\n",
      "[162] cost: 961.234\n",
      "[163] cost: 961.160\n",
      "[164] cost: 961.222\n",
      "[165] cost: 961.360\n",
      "[166] cost: 961.128\n",
      "[167] cost: 961.307\n",
      "[168] cost: 960.942\n",
      "[169] cost: 961.136\n",
      "[170] cost: 961.171\n",
      "[171] cost: 960.859\n",
      "[172] cost: 960.845\n",
      "[173] cost: 961.402\n",
      "[174] cost: 961.405\n",
      "[175] cost: 961.108\n",
      "[176] cost: 960.964\n",
      "[177] cost: 961.446\n",
      "[178] cost: 960.983\n",
      "[179] cost: 960.860\n",
      "[180] cost: 960.964\n",
      "[181] cost: 961.300\n",
      "[182] cost: 960.997\n",
      "[183] cost: 960.948\n",
      "[184] cost: 961.304\n",
      "[185] cost: 961.271\n",
      "[186] cost: 961.221\n",
      "[187] cost: 961.251\n",
      "[188] cost: 961.328\n",
      "[189] cost: 961.120\n",
      "[190] cost: 961.172\n",
      "[191] cost: 960.917\n",
      "[192] cost: 960.881\n",
      "[193] cost: 960.927\n",
      "[194] cost: 961.101\n",
      "[195] cost: 961.461\n",
      "[196] cost: 961.131\n",
      "[197] cost: 961.054\n",
      "[198] cost: 961.225\n",
      "[199] cost: 961.086\n",
      "[200] cost: 961.197\n",
      "[201] cost: 960.986\n",
      "[202] cost: 960.977\n",
      "[203] cost: 960.763\n",
      "[204] cost: 960.852\n",
      "[205] cost: 961.189\n",
      "[206] cost: 961.448\n",
      "[207] cost: 961.193\n",
      "[208] cost: 961.137\n",
      "[209] cost: 961.060\n",
      "[210] cost: 961.208\n",
      "[211] cost: 961.414\n",
      "[212] cost: 961.254\n",
      "[213] cost: 961.301\n",
      "[214] cost: 961.014\n",
      "[215] cost: 960.981\n",
      "[216] cost: 961.285\n",
      "[217] cost: 960.971\n",
      "[218] cost: 961.401\n",
      "[219] cost: 961.089\n",
      "[220] cost: 961.024\n",
      "[221] cost: 961.244\n",
      "[222] cost: 961.096\n",
      "[223] cost: 961.606\n",
      "[224] cost: 961.049\n",
      "[225] cost: 961.026\n",
      "[226] cost: 961.427\n",
      "[227] cost: 961.027\n",
      "[228] cost: 961.071\n",
      "[229] cost: 961.108\n",
      "[230] cost: 961.132\n",
      "[231] cost: 961.332\n",
      "[232] cost: 960.956\n",
      "[233] cost: 960.760\n",
      "[234] cost: 960.953\n",
      "[235] cost: 961.219\n",
      "[236] cost: 961.319\n",
      "[237] cost: 960.927\n",
      "[238] cost: 961.124\n",
      "[239] cost: 961.108\n",
      "[240] cost: 961.151\n",
      "[241] cost: 960.781\n",
      "[242] cost: 960.904\n",
      "[243] cost: 961.364\n",
      "[244] cost: 961.613\n",
      "[245] cost: 961.146\n",
      "[246] cost: 960.809\n",
      "[247] cost: 960.668\n",
      "[248] cost: 960.766\n",
      "[249] cost: 961.053\n",
      "[250] cost: 961.010\n",
      "[251] cost: 960.743\n",
      "[252] cost: 960.879\n",
      "[253] cost: 960.942\n",
      "[254] cost: 961.108\n",
      "[255] cost: 961.049\n",
      "[256] cost: 961.058\n",
      "[257] cost: 961.165\n",
      "[258] cost: 960.878\n",
      "[259] cost: 960.773\n",
      "[260] cost: 961.514\n",
      "[261] cost: 961.429\n",
      "[262] cost: 961.417\n",
      "[263] cost: 961.231\n",
      "[264] cost: 961.111\n",
      "[265] cost: 960.717\n",
      "[266] cost: 961.002\n",
      "[267] cost: 961.433\n",
      "[268] cost: 961.089\n",
      "[269] cost: 960.829\n",
      "[270] cost: 960.692\n",
      "[271] cost: 960.603\n",
      "[272] cost: 961.060\n",
      "[273] cost: 961.095\n",
      "[274] cost: 961.419\n",
      "[275] cost: 961.079\n",
      "[276] cost: 961.013\n",
      "[277] cost: 961.122\n",
      "[278] cost: 961.071\n",
      "[279] cost: 960.975\n",
      "[280] cost: 960.805\n",
      "[281] cost: 961.120\n",
      "[282] cost: 961.335\n",
      "[283] cost: 961.047\n",
      "[284] cost: 960.954\n",
      "[285] cost: 960.889\n",
      "[286] cost: 961.222\n",
      "[287] cost: 961.530\n",
      "[288] cost: 961.140\n",
      "[289] cost: 960.934\n",
      "[290] cost: 960.725\n",
      "[291] cost: 960.778\n",
      "[292] cost: 961.352\n",
      "[293] cost: 961.361\n",
      "[294] cost: 960.921\n",
      "[295] cost: 960.900\n",
      "[296] cost: 961.083\n",
      "[297] cost: 961.208\n",
      "[298] cost: 961.772\n",
      "[299] cost: 961.140\n",
      "[300] cost: 960.933\n",
      "[301] cost: 960.725\n",
      "[302] cost: 960.879\n",
      "[303] cost: 961.249\n",
      "[304] cost: 961.328\n",
      "[305] cost: 960.822\n",
      "[306] cost: 961.087\n",
      "[307] cost: 961.208\n",
      "[308] cost: 961.039\n",
      "[309] cost: 960.756\n",
      "[310] cost: 960.852\n",
      "[311] cost: 960.831\n",
      "[312] cost: 960.957\n",
      "[313] cost: 960.917\n",
      "[314] cost: 961.630\n",
      "[315] cost: 961.288\n",
      "[316] cost: 961.172\n",
      "[317] cost: 961.055\n",
      "[318] cost: 960.930\n",
      "[319] cost: 960.697\n",
      "[320] cost: 960.845\n",
      "[321] cost: 961.043\n",
      "[322] cost: 961.160\n",
      "[323] cost: 961.147\n",
      "[324] cost: 960.870\n",
      "[325] cost: 960.807\n",
      "[326] cost: 960.812\n",
      "[327] cost: 961.071\n",
      "[328] cost: 960.958\n",
      "[329] cost: 960.975\n",
      "[330] cost: 961.110\n",
      "[331] cost: 960.931\n",
      "[332] cost: 960.751\n",
      "[333] cost: 960.931\n",
      "[334] cost: 960.843\n",
      "[335] cost: 960.828\n",
      "[336] cost: 960.936\n",
      "[337] cost: 960.746\n",
      "[338] cost: 960.866\n",
      "[339] cost: 960.977\n",
      "[340] cost: 960.645\n",
      "[341] cost: 961.146\n",
      "[342] cost: 960.809\n",
      "[343] cost: 961.025\n",
      "[344] cost: 961.339\n",
      "[345] cost: 960.880\n",
      "[346] cost: 960.946\n",
      "[347] cost: 961.287\n",
      "[348] cost: 961.143\n",
      "[349] cost: 960.846\n",
      "[350] cost: 960.935\n",
      "[351] cost: 961.039\n",
      "[352] cost: 961.088\n",
      "[353] cost: 960.957\n",
      "[354] cost: 960.903\n",
      "[355] cost: 960.841\n",
      "[356] cost: 961.015\n",
      "[357] cost: 961.250\n",
      "[358] cost: 961.163\n",
      "[359] cost: 961.066\n",
      "[360] cost: 961.050\n",
      "[361] cost: 960.854\n",
      "[362] cost: 960.727\n",
      "[363] cost: 960.924\n",
      "[364] cost: 960.845\n",
      "[365] cost: 960.808\n",
      "[366] cost: 961.022\n",
      "[367] cost: 960.820\n",
      "[368] cost: 960.810\n",
      "[369] cost: 960.883\n",
      "[370] cost: 961.159\n",
      "[371] cost: 960.980\n",
      "[372] cost: 961.065\n",
      "[373] cost: 961.278\n",
      "[374] cost: 961.056\n",
      "[375] cost: 960.801\n",
      "[376] cost: 961.085\n",
      "[377] cost: 960.925\n",
      "[378] cost: 960.853\n",
      "[379] cost: 960.929\n",
      "[380] cost: 960.773\n",
      "[381] cost: 960.799\n",
      "[382] cost: 961.091\n",
      "[383] cost: 960.810\n",
      "[384] cost: 961.082\n",
      "[385] cost: 961.565\n",
      "[386] cost: 961.203\n",
      "[387] cost: 960.885\n",
      "[388] cost: 961.021\n",
      "[389] cost: 960.780\n",
      "[390] cost: 960.980\n",
      "[391] cost: 961.176\n",
      "[392] cost: 960.982\n",
      "[393] cost: 961.131\n",
      "[394] cost: 961.179\n",
      "[395] cost: 960.823\n",
      "[396] cost: 960.985\n",
      "[397] cost: 961.093\n",
      "[398] cost: 961.128\n",
      "[399] cost: 961.254\n",
      "[400] cost: 961.243\n",
      "[401] cost: 960.893\n",
      "[402] cost: 960.735\n",
      "[403] cost: 960.656\n",
      "[404] cost: 960.864\n",
      "[405] cost: 961.227\n",
      "[406] cost: 961.312\n",
      "[407] cost: 961.308\n",
      "[408] cost: 960.991\n",
      "[409] cost: 960.991\n",
      "[410] cost: 960.977\n",
      "[411] cost: 960.802\n",
      "[412] cost: 960.893\n",
      "[413] cost: 960.668\n",
      "[414] cost: 960.756\n",
      "[415] cost: 961.057\n",
      "[416] cost: 960.892\n",
      "[417] cost: 961.232\n",
      "[418] cost: 961.126\n",
      "[419] cost: 961.421\n",
      "[420] cost: 961.177\n",
      "[421] cost: 960.720\n",
      "[422] cost: 960.907\n",
      "[423] cost: 960.710\n",
      "[424] cost: 960.833\n",
      "[425] cost: 961.142\n",
      "[426] cost: 960.945\n",
      "[427] cost: 960.636\n",
      "[428] cost: 960.705\n",
      "[429] cost: 961.323\n",
      "[430] cost: 961.325\n",
      "[431] cost: 961.023\n",
      "[432] cost: 960.939\n",
      "[433] cost: 960.821\n",
      "[434] cost: 960.626\n",
      "[435] cost: 960.918\n",
      "[436] cost: 960.918\n",
      "[437] cost: 961.253\n",
      "[438] cost: 961.305\n",
      "[439] cost: 961.033\n",
      "[440] cost: 960.766\n",
      "[441] cost: 960.672\n",
      "[442] cost: 961.086\n",
      "[443] cost: 960.867\n",
      "[444] cost: 960.627\n",
      "[445] cost: 960.697\n",
      "[446] cost: 960.823\n",
      "[447] cost: 960.974\n",
      "[448] cost: 960.846\n",
      "[449] cost: 961.085\n",
      "[450] cost: 961.034\n",
      "[451] cost: 961.059\n",
      "[452] cost: 961.194\n",
      "[453] cost: 960.990\n",
      "[454] cost: 960.804\n",
      "[455] cost: 960.854\n",
      "[456] cost: 960.635\n",
      "[457] cost: 960.841\n",
      "[458] cost: 960.815\n",
      "[459] cost: 960.701\n",
      "[460] cost: 961.120\n",
      "[461] cost: 960.782\n",
      "[462] cost: 960.880\n",
      "[463] cost: 960.782\n",
      "[464] cost: 960.697\n",
      "[465] cost: 960.809\n",
      "[466] cost: 961.201\n",
      "[467] cost: 961.152\n",
      "[468] cost: 960.639\n",
      "[469] cost: 960.593\n",
      "[470] cost: 960.794\n",
      "[471] cost: 960.647\n",
      "[472] cost: 960.672\n",
      "[473] cost: 960.931\n",
      "[474] cost: 961.105\n",
      "[475] cost: 961.326\n",
      "[476] cost: 960.982\n",
      "[477] cost: 961.084\n",
      "[478] cost: 960.782\n",
      "[479] cost: 960.872\n",
      "[480] cost: 960.907\n",
      "[481] cost: 961.097\n",
      "[482] cost: 960.961\n",
      "[483] cost: 961.369\n",
      "[484] cost: 960.784\n",
      "[485] cost: 960.686\n",
      "[486] cost: 960.567\n",
      "[487] cost: 961.335\n",
      "[488] cost: 961.097\n",
      "[489] cost: 961.331\n",
      "[490] cost: 961.328\n",
      "[491] cost: 961.255\n",
      "[492] cost: 961.091\n",
      "[493] cost: 961.141\n",
      "[494] cost: 961.103\n",
      "[495] cost: 960.829\n",
      "[496] cost: 961.075\n",
      "[497] cost: 960.859\n",
      "[498] cost: 960.756\n",
      "[499] cost: 961.466\n",
      "[500] cost: 961.068\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "costs=[]\n",
    "for epoch in range(500):\n",
    "    cost=0.0\n",
    "    for i,train in enumerate(train_loader):\n",
    "        X_train,y_train=train\n",
    "        X_train=X_train.to(device)\n",
    "        y_train=y_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred=cnn(X_train)\n",
    "        loss=criterion(y_pred,y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    costs.append(cost)\n",
    "    print('[%d] cost: %.3f'%(epoch+1,cost))\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0.975452380952381"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac=0\n",
    "for X_train,y_train in iter(train_loader.dataset):\n",
    "    y_pred=cnn(X_train.to(device).reshape(1,1,28,28)).to(torch.int)\n",
    "    y_train=y_train.reshape(1,10).to(device,torch.int)\n",
    "    if (y_pred==y_train).sum()==10:\n",
    "        ac+=1\n",
    "ac/42000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(),'saves/kaggle_cnn_params.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure('PyTorch_CNN_Cost')\n",
    "# plt.plot(costs,label='Cost')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# 被阻塞了，不画了，以后再说\n",
    "# 据说貌似一口气运行就会出现 plt 阻塞的情况，然后 kernel 就炸了"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于交叉熵的代价函数，那么大大概很正常，-ylogy 加和后的大小主要来源是接近 0 的 y 吧\n",
    "但是别看那么大，显然下降一点点实际上影响其实就会很明显\n",
    "然后实际上大概不用跑这么多，我感觉 epoch=80 的时候就已经收敛得差不多了"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}