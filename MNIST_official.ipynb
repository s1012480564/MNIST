{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 用torchvision自带的官方图片数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "参考代码：https://blog.csdn.net/qq_34714751/article/details/85610966"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import mnist\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "读入数据增强后的，这里做了归一化，均值0.5，方差0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "trans=transforms.Compose([transforms.ToTensor(),\n",
    "                          transforms.Normalize([0.5],[0.5])]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里后来查了下，原来 transforms.ToTensor() 是除以 255 且转 tensor\n",
    "然后 transforms.Normalize -0.5/0.5 归一化到了 [-1,1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# num_workers 默认 = 0，我们知道加载 batch 是到 RAM 中找，所以 batch 大小很有讲究，这样就可以避免频繁缺块\n",
    "# 但是注意我们这样还是得找，如果指定 worker 来负责读写的这个任务就会快一些\n",
    "# num_workers 可设置的大小取决于 CPU 逻辑核心数\n",
    "train_set=mnist.MNIST('official_data',train=True,transform=trans)\n",
    "test_set=mnist.MNIST('official_data',train=False,transform=trans)\n",
    "train_loader=DataLoader(train_set,batch_size=64,shuffle=True,num_workers=4)\n",
    "test_loader=DataLoader(test_set,batch_size=64,shuffle=True,num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: official_data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=[0.5], std=[0.5])\n           )"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对一个迭代类型，当你不知道怎么读时\n",
    "一定可以用 enumerate 或者 iter 暴力读出，参考代码：\n",
    "for i,xy in enumerate(train):\n",
    "    x,y=xy\n",
    "很神奇的是 tensor 没有 ndarray, tuple 之类的类型，全是混在一起的，但是可以用那样的用法，就像是潜在的类型\n",
    "比如上面可能是一个数字5\n",
    "可能一个样本的数据是这样(注意一个样本，丢了batch维，然后是单通道维，然后是行维、列维)\n",
    "维度 B、C、H、W，注意 B、C 所处的位置\n",
    "tensor(\\[\\[\\[1,2\\],\\[2,3\\]\\]\\],5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LeNet-5架构(无修正版)：当然还是有一点修正，运行时发现这个数据和吴恩达说好的不一样，怎么是 28\\*28 的，不是32\\*32，于是我在下一种写法里加了 padding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (conv1): Sequential(\n    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (fc1): Sequential(\n    (0): Linear(in_features=400, out_features=120, bias=True)\n    (1): ReLU()\n  )\n  (fc2): Sequential(\n    (0): Linear(in_features=120, out_features=84, bias=True)\n    (1): ReLU()\n  )\n  (out): Linear(in_features=84, out_features=1, bias=True)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        # 注意，也不要忘了偏置单元的事，一般默认都是 bias=True，所以没事\n",
    "        self.conv1=nn.Sequential(\n",
    "            # nn.Conv2d(in_channels=1,\n",
    "            #           out_channels=6,\n",
    "            #           kernel_size=5),\n",
    "            nn.Conv2d(1,6,5), # 默认参数 in_channels,out_channels,kernel_size,stride=1,padding=0\n",
    "            nn.ReLU(),\n",
    "            # nn.AvgPool2d(kernel_size=2,\n",
    "            #              stride=2)\n",
    "            nn.AvgPool2d(2,2) # 默认参数 kernel_size,stride=0,padding=0\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(6,16,5),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2,2)\n",
    "        )\n",
    "        self.fc1=nn.Sequential(\n",
    "            nn.Linear(400,120), # in_features, out_features 输出特征数就是本层结点数\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2=nn.Sequential(\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out=nn.Linear(84,1)\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        # -1 表示自动计算，比如总共 8，-1*4 就是 2*4 的意思，这里 -1 实际上就是 batch size\n",
    "        x=x.view(-1,self.num_flat_features(x))\n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        x=self.out(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x): # 这个 flatten 就和模板一样，以后用连改都不用改\n",
    "        size=x.size()[1:]  # m*5*5*16，第一维是 batch size，每个样本 flatten 成 5*5*16=400 的列向量\n",
    "        num_features=1\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "cnn=CNN()\n",
    "cnn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "或者，下面这种写法更常用一些，一些fuctional写在前向传播里"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (out): Linear(in_features=84, out_features=1, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5,padding=2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.pool=nn.AvgPool2d(2,2)\n",
    "        self.fc1=nn.Linear(400,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.out=nn.Linear(84,1)\n",
    "    def forward(self,x):\n",
    "        # 这也是一种写法\n",
    "        # x=F.avg_pool2d(F.relu(self.conv1(x)),2) # F的pool默认参数有点区别，这里是默认 stride=kernel_size\n",
    "        # x=F.avg_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,self.num_flat_features(x)) # 听说 torch 的 reshape 比 view 功能更多，没具体了解，以后还是写reshape吧\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.out(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size=x.size()[1:]\n",
    "        num_features=1\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "cnn=CNN()\n",
    "cnn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将模型和张量移动到GPU，张量的移动写在下面的梯度下降函数里\n",
    "我只有一个英伟达的，另一个用不了CUDA，所以这里没用 nn.DataParallel 多 GPUs\n",
    "（讲真，GPU实在太快了，这一个GPU都比我的四核八线程CPU强好多）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')\n",
    "cnn=cnn.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义损失函数\n",
    "这个古董版本输出层用的是 RELU，而非 Softmax\n",
    "第一次写DL代码，令我感到震惊的是，你写逻辑回归的交叉熵损失函数CrossEntropyLoss()它也能用\n",
    "牛头对马嘴他也能跑，但是逻辑说不通，结果的话训练得多了也不会很差。。。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(cnn.parameters()) # 按照吴恩达的建议，Adam优化器超参数直接默认就完事"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] cost: 2609.389\n",
      "[2] cost: 715.124\n",
      "[3] cost: 475.607\n",
      "[4] cost: 380.799\n",
      "[5] cost: 321.586\n",
      "[6] cost: 275.797\n",
      "[7] cost: 241.117\n",
      "[8] cost: 212.992\n",
      "[9] cost: 188.990\n",
      "[10] cost: 171.879\n",
      "[11] cost: 155.069\n",
      "[12] cost: 144.302\n",
      "[13] cost: 127.231\n",
      "[14] cost: 118.793\n",
      "[15] cost: 119.160\n",
      "[16] cost: 103.842\n",
      "[17] cost: 96.602\n",
      "[18] cost: 93.058\n",
      "[19] cost: 82.451\n",
      "[20] cost: 82.932\n",
      "[21] cost: 73.408\n",
      "[22] cost: 65.177\n",
      "[23] cost: 63.881\n",
      "[24] cost: 65.848\n",
      "[25] cost: 60.186\n",
      "[26] cost: 51.232\n",
      "[27] cost: 52.346\n",
      "[28] cost: 48.810\n",
      "[29] cost: 47.064\n",
      "[30] cost: 42.029\n",
      "[31] cost: 43.261\n",
      "[32] cost: 39.202\n",
      "[33] cost: 40.610\n",
      "[34] cost: 37.024\n",
      "[35] cost: 40.194\n",
      "[36] cost: 31.233\n",
      "[37] cost: 27.847\n",
      "[38] cost: 27.205\n",
      "[39] cost: 42.675\n",
      "[40] cost: 30.602\n",
      "[41] cost: 26.491\n",
      "[42] cost: 22.011\n",
      "[43] cost: 27.917\n",
      "[44] cost: 31.764\n",
      "[45] cost: 28.367\n",
      "[46] cost: 20.938\n",
      "[47] cost: 20.407\n",
      "[48] cost: 24.735\n",
      "[49] cost: 27.552\n",
      "[50] cost: 17.990\n",
      "[51] cost: 24.275\n",
      "[52] cost: 15.903\n",
      "[53] cost: 32.545\n",
      "[54] cost: 20.132\n",
      "[55] cost: 15.042\n",
      "[56] cost: 17.866\n",
      "[57] cost: 17.939\n",
      "[58] cost: 23.047\n",
      "[59] cost: 18.012\n",
      "[60] cost: 15.048\n",
      "[61] cost: 16.767\n",
      "[62] cost: 13.474\n",
      "[63] cost: 18.474\n",
      "[64] cost: 11.437\n",
      "[65] cost: 9.529\n",
      "[66] cost: 16.669\n",
      "[67] cost: 13.919\n",
      "[68] cost: 16.438\n",
      "[69] cost: 15.611\n",
      "[70] cost: 7.923\n",
      "[71] cost: 15.554\n",
      "[72] cost: 10.682\n",
      "[73] cost: 11.553\n",
      "[74] cost: 10.419\n",
      "[75] cost: 13.531\n",
      "[76] cost: 17.673\n",
      "[77] cost: 7.918\n",
      "[78] cost: 11.570\n",
      "[79] cost: 16.136\n",
      "[80] cost: 12.778\n",
      "[81] cost: 6.659\n",
      "[82] cost: 8.251\n",
      "[83] cost: 13.235\n",
      "[84] cost: 10.626\n",
      "[85] cost: 13.451\n",
      "[86] cost: 8.942\n",
      "[87] cost: 10.094\n",
      "[88] cost: 8.965\n",
      "[89] cost: 10.316\n",
      "[90] cost: 11.502\n",
      "[91] cost: 7.030\n",
      "[92] cost: 10.270\n",
      "[93] cost: 5.491\n",
      "[94] cost: 13.204\n",
      "[95] cost: 11.340\n",
      "[96] cost: 6.727\n",
      "[97] cost: 5.525\n",
      "[98] cost: 16.445\n",
      "[99] cost: 5.350\n",
      "[100] cost: 5.766\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    # 遍历 train_loader，其实是在遍历 batch，这里我们 batch_size=64 于是 60000/64=938 个 batch\n",
    "    # 不用 mini-batch 的话就是 1 个 epoch 里用 1 个 batch\n",
    "    cost=0.0\n",
    "    for i,train in enumerate(train_loader):\n",
    "        X_train,y_train=train\n",
    "        X_train=X_train.to(device) # 写成 X_train.cuda() 就是默认仅放在 gpu:0 上\n",
    "        y_train=y_train.to(device)\n",
    "        y_train=y_train.to(torch.float32) # 离谱，torch 连 long 和 float 自动转换都做不到，原始标签是 long\n",
    "        y_train=y_train.view(y_train.size()[0],1) # 维度也得自己改改，离谱啊\n",
    "        optimizer.zero_grad() # 初始化梯度为 0，因为是链式求导，会累乘，而我们不要上一个 batch 的\n",
    "        y_pred=cnn(X_train)\n",
    "        loss=criterion(y_pred,y_train) # 注意线性激活算MSE，两个都得是float类型\n",
    "        loss.backward() # 反向传播计算梯度\n",
    "        optimizer.step() # 用梯度更新\n",
    "        cost+=loss.item()\n",
    "    print('[%d] cost: %.3f'%(epoch+1,cost))\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(5.)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=train_loader.dataset[0][1]\n",
    "y=torch.tensor(data=y,dtype=torch.float32)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 28, 28])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for train in iter(train_loader):\n",
    "    print(train[0].shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如上所示\n",
    "dataset[i] 可以取出(X,y)，而且是严格的一条数据，当然注意掉维问题\n",
    "我们知道如果你迭代 train_loader 的话取出的是 batch_size 个合并的一条\n",
    "还要注意的是 DataLoader 的 batch_size 参数默认值是 1，而非样本点个数\n",
    "这意味着默认的是 SGD 随机梯度下降，而非 batch，更不是 mini-batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 28, 28])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=train_loader.dataset[0][0]\n",
    "x=x.view(-1,x.size()[0],x.size()[1],x.size()[2])\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "x=x.to(device)\n",
    "y_pred=cnn(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5.0036]], device='cuda:0', grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里我们保存一下训练好的网络，避免重复训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(),'saves/official_cnn_params.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "另一种保存方式是直接 torch.save(cnn,路径)\n",
    "然后这个就是保存整个 cnn 对象的内容，就很慢很大了\n",
    "我们上面的写法是仅保存了参数\n",
    "当然保存参数缺点是你还要重新创建对象，设置一些属性之类的\n",
    "实际上 torch 的 save 本质上应该和 python 的 pickle 或者 shelve 是一样的\n",
    "就是把对象内容保存为一个二进制文件\n",
    "然后关于后缀，其实是啥都无所谓，因为不对应任何直接打开的格式\n",
    "习惯性使用 .pt .pth 或者 .pkl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}